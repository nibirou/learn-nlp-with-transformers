{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882ab46a",
   "metadata": {},
   "source": [
    "## 图解transformer\n",
    "\n",
    "内容组织：\n",
    "- 图解transformer\n",
    "    - Transformer宏观结构\n",
    "    - Transformer结构细节\n",
    "      - 输入处理\n",
    "        - 词向量\n",
    "        - 位置向量\n",
    "      - 编码器encoder\n",
    "        - Self-Attention层\n",
    "        - 多头注意力机制\n",
    "        - Attention代码实例\n",
    "        - 残差连接\n",
    "      - 解码器\n",
    "      - 线性层和softmax\n",
    "      - 损失函数\n",
    "    - 附加资料\n",
    "    - 致谢\n",
    "\n",
    "在学习完[图解attention](./2.1-图解attention.md)后，我们知晓了attention为循环神经网络带来的优点。那么有没有一种神经网络结构直接基于attention构造，并且不再依赖RNN、LSTM或者CNN网络结构了呢？答案便是：Transformer。因此，我们将在本小节对Transformer所涉及的细节进行深入探讨。\n",
    "\n",
    "Transformer模型在2017年被google提出，直接基于Self-Attention结构，取代了之前NLP任务中常用的RNN神经网络结构，并在WMT2014 Englishto-German和WMT2014 English-to-French两个机器翻译任务上都取得了当时的SOTA。\n",
    "\n",
    "与RNN这类神经网络结构相比，Transformer一个巨大的优点是：**模型在处理序列输入时，可以对整个序列输入进行并行计算，不需要按照时间步循环递归处理输入序列。**2.1章节详细介绍了RNN神经网络如何循环递归处理输入序列，欢迎读者复习查阅。\n",
    "\n",
    "下图先便是Transformer整体结构图，与篇章2.1中介绍的seq2seq模型类似，Transformer模型结构中的左半部分为编码器（encoder），右半部分为解码器（decoder），下面我们来一步步拆解 Transformer。\n",
    "\n",
    "![transformer](./pictures/2-transformer.png)\n",
    "图：transformer模型结构\n",
    "\n",
    "注释和引用说明：本文将通过总-分的方式对Transformer进行拆解和讲解，希望有助于帮助初学者理解Transformer模型结构。本文主要参考[illustrated-transformer](http://jalammar.github.io/illustrated-transformer)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce91c8",
   "metadata": {},
   "source": [
    "## Transformer宏观结构\n",
    "\n",
    "注意：本小节为讲解方式为：总-分，先整体，再局部。\n",
    "\n",
    "Transformer最开始提出来解决机器翻译任务，因此可以看作是seq2seq模型的一种。本小节先抛开Transformer模型中结构具体细节，先从seq2seq的角度对Transformer进行宏观结构的学习。以机器翻译任务为例，先将Transformer这种特殊的seqseq模型看作一个黑盒，黑盒的输入是法语文本序列，输出是英语文本序列（对比2.1章节的seq2seq框架知识我们可以发现，Transformer宏观结构属于seq2seq范畴，只是将之前seq2seq中的编码器和解码器，从RNN模型替换成了Transformer模型）。\n",
    "\n",
    "![input-output](./pictures/2-input-output.png)\n",
    "图：Transformer黑盒输入和输出\n",
    "\n",
    "将上图中的中间部分“THE TRANSFORMER”拆开成seq2seq标准结构，得到下图：左边是编码部分encoders，右边是解码器部分decoders。\n",
    "![encoder-decoder](./pictures/2-encoder-decoder.png)\n",
    "图：encoder-decoder\n",
    "\n",
    "下面，再将上图中的编码器和解码器细节绘出，得到下图。我们可以看到，编码部分（encoders）由多层编码器(Encoder)组成（Transformer论文中使用的是6层编码器，这里的层数6并不是固定的，你也可以根据实验效果来修改层数）。同理，解码部分（decoders）也是由多层的解码器(Decoder)组成（论文里也使用了6层解码器）。每层编码器网络结构是一样的，每层解码器网络结构也是一样的。不同层编码器和解码器网络结构不共享参数。\n",
    "![翻译例子](./pictures/2-2-encoder-detail.png)\n",
    "\n",
    "图：6层编码和6层解码器\n",
    "\n",
    "接下来，我们看一下单层encoder，单层encoder主要由以下两部分组成，如下图所示\n",
    "- Self-Attention Layer\n",
    "- Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）\n",
    "\n",
    "编码器的输入文本序列$w_1, w_2,...,w_n$最开始需要经过embedding转换，得到每个单词的向量表示$x_1, x_2,...,x_n$，其中$x_i \\in \\mathbb{R}^{d}$是维度为$d$的向量，然后所有向量经过一个Self-Attention神经网络层进行变换和信息交互得到$h_1, h_2,...h_n$，其中$h_i \\in \\mathbb{R}^{d}$是维度为$d$的向量。self-attention层处理一个词向量的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息（你可以类比为：当我们翻译一个词的时候，不仅会只关注当前的词，也会关注这个词的上下文的其他词的信息）。Self-Attention层的输出会经过前馈神经网络得到新的$x_1, x_2,..,x_n$，依旧是$n$个维度为$d$的向量。这些向量将被送入下一层encoder，继续相同的操作。\n",
    "\n",
    "![encoder](./pictures/2-encoder.png)\n",
    "\n",
    "图：单层encoder\n",
    "\n",
    "与编码器对应，如下图，解码器在编码器的self-attention和FFNN中间插入了一个Encoder-Decoder Attention层，这个层帮助解码器聚焦于输入序列最相关的部分（类似于seq2seq模型中的 Attention）。\n",
    "\n",
    "![decoder](./pictures/2-decoder.webp)\n",
    "\n",
    "图：单层decoder\n",
    "\n",
    "总结一下，我们基本了解了Transformer由编码部分和解码部分组成，而编码部分和解码部分又由多个网络结构相同的编码层和解码层组成。每个编码层由self-attention和FFNN组成，每个解码层由self-attention、FFN和encoder-decoder attention组成。\n",
    "\n",
    "以上便是Transformer的宏观结构啦，下面我们开始看宏观结构中的模型细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c95176",
   "metadata": {},
   "source": [
    "## Transformer结构细节\n",
    "\n",
    "了解了Transformer的宏观结构之后。下面，让我们来看看Transformer如何将输入文本序列转换为向量表示，又如何逐层处理这些向量表示得到最终的输出。\n",
    "\n",
    "因此本节主要内容依次包含：\n",
    "- 输入处理\n",
    "  - 词向量\n",
    "  - 位置向量\n",
    "- 编码器\n",
    "- 解码器\n",
    "\n",
    "### 输入处理\n",
    "\n",
    "#### 词向量\n",
    "和常见的NLP 任务一样，我们首先会使用词嵌入算法（embedding algorithm），将输入文本序列的每个词转换为一个词向量。实际应用中的向量一般是 256 或者 512 维。但为了简化起见，我们这里使用4维的词向量来进行讲解。\n",
    "\n",
    "如下图所示，假设我们的输入文本是序列包含了3个词，那么每个词可以通过词嵌入算法得到一个4维向量，于是整个输入被转化成为一个向量序列。在实际应用中，我们通常会同时给模型输入多个句子，如果每个句子的长度不一样，我们会选择一个合适的长度，作为输入文本序列的最大长度：如果一个句子达不到这个长度，那么就填充先填充一个特殊的“padding”词；如果句子超出这个长度，则做截断。最大序列长度是一个超参数，通常希望越大越好，但是更长的序列往往会占用更大的训练显存/内存，因此需要在模型训练时候视情况进行决定。\n",
    "\n",
    "![ 个词向量](./pictures/2-x.png)\n",
    "图：3个词和对应的词向量\n",
    "\n",
    "输入序列每个单词被转换成词向量表示还将加上位置向量来得到该词的最终向量表示。\n",
    "\n",
    "#### 位置向量\n",
    "\n",
    "如下图所示，Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。\n",
    "\n",
    "![位置编码](./pictures/2-position.png)\n",
    "图：位置编码向量\n",
    "\n",
    "依旧假设词向量和位置向量的维度是4，我们在下图中展示一种可能的位置向量+词向量：\n",
    "\n",
    "![位置编码](./pictures/2-position2.png)\n",
    "图：位置编码向量\n",
    "\n",
    "那么带有位置编码信息的向量到底遵循什么模式？原始论文中给出的设计表达式为：\n",
    "$$\n",
    "PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                       PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}}) \n",
    "$$\n",
    "上面表达式中的$pos$代表词的位置，$d_{model}$代表位置向量的维度，$i \\in [0, d_{model})$代表位置$d_{model}$维位置向量第$i$维。于是根据上述公式，我们可以得到第$pos$位置的$d_{model}$维位置向量。在下图中，我们画出了一种位置向量在第4、5、6、7维度、不同位置的的数值大小。横坐标表示位置下标，纵坐标表示数值大小。\n",
    "\n",
    "![位置编码图示](./pictures/2-2-pos-embedding.png)\n",
    "图：位置编码在0-100位置，在4、5、6、7维的数值图示\n",
    "\n",
    "当然，上述公式不是唯一生成位置编码向量的方法。但这种方法的优点是：可以扩展到未知的序列长度。例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d179f",
   "metadata": {},
   "source": [
    "### 编码器encoder\n",
    "\n",
    "编码部分的输入文本序列经过输入处理之后得到了一个向量序列，这个向量序列将被送入第1层编码器，第1层编码器输出的同样是一个向量序列，再接着送入下一层编码器：第1层编码器的输入是融合位置向量的词向量，*更上层编码器的输入则是上一层编码器的输出*。\n",
    "\n",
    "下图展示了向量序列在单层encoder中的流动：融合位置信息的词向量进入self-attention层，self-attention的输出每个位置的向量再输入FFN神经网络得到每个位置的新向量。\n",
    "\n",
    "![输入encoder](./pictures/2-x-encoder.png)\n",
    "图：单层encoder的序列向量流动\n",
    "\n",
    "下面再看一个2个单词的例子：\n",
    "![一层传一层](./pictures/2-multi-encoder.webp)\n",
    "图：2个单词的例子：$x_1, x_2 \\to z_1, z_2 \\to r_1, r_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe36a3a",
   "metadata": {},
   "source": [
    "### Self-Attention层\n",
    "\n",
    "下面来分析一下上图中Self-Attention层的具体机制。\n",
    "\n",
    "##### Self-Attention概览 \n",
    "\n",
    "假设我们想要翻译的句子是：\n",
    "```\n",
    "The animal didn't cross the street because it was too tired\n",
    "```\n",
    "这个句子中的 *it* 是一个指代词，那么 *it* 指的是什么呢？它是指 *animal* 还是*street*？这个问题对人来说，是很简单的，但是对模型来说并不是那么容易。但是，如果模型引入了*Self Attention*机制之后，便能够让模型把it和animal关联起来了。同样的，当模型处理句子中其他词时，*Self Attentio*n机制也可以使得模型不仅仅关注当前位置的词，还会关注句子中其他位置的相关的词，进而可以更好地理解当前位置的词。\n",
    "\n",
    "与2.1章节中提到的RNN对比一下：RNN 在处理序列中的一个词时，会考虑句子前面的词传过来的*hidden state*，而*hidden state*就包含了前面的词的信息；而*Self Attention*机制值得是，当前词会直接关注到自己句子中前后相关的所有词语，如下图 *it*的例子：\n",
    "\n",
    "![一个词和其他词的attention](./pictures/2-attention-word.png)\n",
    "\n",
    "图：一个词和其他词的attention\n",
    "\n",
    "上图所示的*it*是一个真实的例子，是当Transformer在第5层编码器编码“it”时的状态，可视化之后显示*it*有一部分注意力集中在了“The animal”上，并且把这两个词的信息融合到了\"it\"中。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
